Frontend:
  served_model_name: meta-llama/Llama-3.1-8B-Instruct
  endpoint: dynamo.Processor.chat/completions
  port: 8000

Processor:
  model: /data/models/Llama-3.1-8B-Instruct
  router: round-robin
  trust-remote-code: true

VllmWorker:
  # vllm enging args
  model: /data/models/Llama-3.1-8B-Instruct
  served_model_name: meta-llama/Llama-3.1-8B-Instruct
  kv-transfer-config: '{"kv_connector":"DynamoNixlConnector"}'
  trust-remote-code: true
  block-size: 64
  max-model-len: 32768
  max-num-batched-tokens: 32768
  speculative-model: /data/models/Llama-3.2-1B
  num-speculative-tokens: 1  # enable MTP
  tensor-parallel-size: 1
  enforce-eager: true
  gpu-memory-utilization: 0.90
  # dynamo args
  remote-prefill: true
  conditional-disagg: true
  max-local-prefill-length: 64
  ServiceArgs:
    workers: 1
    resources:
      gpu: 1
